{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tfidf_matcher as tm\n",
    "import datetime as dt\n",
    "import time\n",
    "import calendar\n",
    "from difflib import SequenceMatcher\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 545 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vmf_df= pd.read_csv('VMF_vendor_list.csv', sep='\\t', encoding='utf-16')\n",
    "access_rights_df = pd.read_csv('VMF_access_rights.csv', sep='\\t', encoding='utf-16').dropna(how='all')\n",
    "employees_df = pd.read_csv('VMF_employee_list.csv', sep='\\t', encoding='utf-16').dropna(how='all')\n",
    "terminated_employees_df = pd.read_csv('VMF_terminated_employees.csv', sep='\\t', encoding='utf-16').dropna(how='all')\n",
    "po_df = pd.read_csv('VMF_po_list.csv', sep='\\t', encoding='utf-16').dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Applying proper dtypes\n",
    "\n",
    "vmf_df.name = vmf_df.name.astype(str)\n",
    "vmf_df.creation_date = pd.to_datetime(vmf_df.creation_date)\n",
    "vmf_df.modification_date = pd.to_datetime(vmf_df.modification_date)\n",
    "\n",
    "employees_df.employee_name = employees_df.employee_name.astype(str)\n",
    "employees_df.hiring_date = pd.to_datetime(employees_df.hiring_date)\n",
    "\n",
    "terminated_employees_df.employee_name = terminated_employees_df.employee_name.astype(str)\n",
    "terminated_employees_df.hiring_date = pd.to_datetime(terminated_employees_df.hiring_date)\n",
    "terminated_employees_df.termination_date = pd.to_datetime(terminated_employees_df.termination_date)\n",
    "\n",
    "po_df.po_date = pd.to_datetime(po_df.po_date)\n",
    "po_df.po_total = po_df.po_total.str.replace(',', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(df, col1, col2):\n",
    "    '''calculate similarities of returned results from tfidf matcher\n",
    "    to facilitate filtering results'''\n",
    "    return round(SequenceMatcher(None, df[col1], df[col2]).ratio(),2)\n",
    "\n",
    "def similar_with_nan(df, col1, col2):\n",
    "    '''calculate similarities between two dataframe columns while ignoring nan values'''\n",
    "    if not df[[col1,col2]].isnull().any():\n",
    "        return round(SequenceMatcher(None, df[col1], df[col2]).ratio(),2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def non_english_names(name):\n",
    "    return name.isascii()\n",
    "\n",
    "n_gram = 3\n",
    "n_matches = 10\n",
    "\n",
    "def employee_vs_vendor_records(vendor_df, employee_df, n_gram, n_matches):\n",
    "    '''finding exact and fuzzy matches between employee and vendor names \n",
    "    based on user specified n_gram and number of desired matches 'n_matches'.\n",
    "    n_gram and n_matches default values are 3 and 10 respectively which mostly yield best results\n",
    "    results are not filtered but are sorted in a descending order for convenience.\n",
    "    '''\n",
    "    \n",
    "    # identifying which datafram is loaded to properly name results while saving\n",
    "    # as this function will be applied for both active and terminated employees\n",
    "    \n",
    "    employee_status = ''\n",
    "    \n",
    "    if employee_df is employees_df:\n",
    "        employee_status= 'active'\n",
    "    else:\n",
    "        employee_status= 'terminated'\n",
    "        \n",
    "    # preparing list of names to match\n",
    "    vendor_names_list = vendor_df.name.tolist()\n",
    "    \n",
    "    employee_names_list = employee_df.employee_name.tolist()\n",
    "    \n",
    "    # applying match function, adjusting data view and calculating similarities of returned results\n",
    "    em_name_match_df=tm.matcher(employee_names_list, vendor_names_list, ngram_length= n_gram, k_matches= n_matches)\n",
    "\n",
    "    em_name_match_df=pd.merge(em_name_match_df,employee_df[['employee_id']], left_index=True,right_index=True,how='left')\n",
    "\n",
    "    em_name_match_df.drop_duplicates(subset='Original Name', inplace=True)\n",
    "\n",
    "    em_name_match_df.drop(columns=['Match Confidence'], inplace=True)\n",
    "\n",
    "    em_name_match_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    em_name_match_df=em_name_match_df.melt(id_vars=[\"Original Name\", 'employee_id'],ignore_index=False)\n",
    "\n",
    "    em_name_match_df.drop(columns=['variable'], inplace=True)\n",
    "    \n",
    "    em_name_match_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    em_name_match_df['similarity']=em_name_match_df.apply(similar,args=('Original Name', 'value'),axis=1)\n",
    "\n",
    "    em_name_match_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    em_name_match_df.rename(columns={'value':'name'}, inplace=True)\n",
    "\n",
    "    em_name_match_df.index=em_name_match_df.name\n",
    "\n",
    "    vendor_df.index=vendor_df.name\n",
    "    \n",
    "    # collecting vendor details for matched vendors\n",
    "    \n",
    "    em_name_match_df=pd.merge(em_name_match_df,vendor_df[['id', 'vendor_status']], left_index=True,right_index=True,how='left')\n",
    "\n",
    "    em_name_match_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    vendor_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # removing reverse duplicate matches to eliminate data redundancy, \n",
    "    # i.e: match result of x,y and y,x basically refer to same records\n",
    "    # first we join and sort both names to create a unique value for each result\n",
    "    # then drop duplicate joined names\n",
    "    \n",
    "    em_name_match_df['reverse_duplicates'] = em_name_match_df.apply(lambda row: ' '.join(sorted([row['Original Name'], row['name']])), axis=1)\n",
    "\n",
    "    em_name_match_df.drop_duplicates(subset='reverse_duplicates', inplace=True)\n",
    "\n",
    "    em_name_match_df.drop(columns='reverse_duplicates', inplace=True)\n",
    "    \n",
    "    em_name_match_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # collecting all employee/vendor details and sorting data for better view and comparison\n",
    "    \n",
    "    em_name_match_df=pd.merge(em_name_match_df,employee_df[['employee_id', 'phone', 'postal_code', 'address', \n",
    "                                                             'social_security_number_ssn']], on='employee_id', how='left')\n",
    "\n",
    "    em_name_match_df.rename(columns={'Original Name':'employee_name', 'phone':'employee_phone', 'postal_code':'employee_postal_code', 'address':'employee_address',\n",
    "                                     'social_security_number_ssn':'employee_ssn'}, inplace=True)\n",
    "\n",
    "    em_name_match_df=pd.merge(em_name_match_df,vendor_df[['id', 'phone', 'postal_code', 'address', \n",
    "                                                       'taxpayer_identification_number_tin']], on='id', how='left')\n",
    "\n",
    "    em_name_match_df.rename(columns={'id':'vendor_id', 'name':'vendor_name', 'phone':'vendor_phone', \n",
    "                                     'postal_code':'vendor_postal_code', 'address':'vendor_address', \n",
    "                                     'taxpayer_identification_number_tin':'vendor_tin'}, inplace=True)\n",
    "\n",
    "    result_columns=em_name_match_df.columns.tolist()\n",
    "    columns_sort=result_columns[1:2]+result_columns[0:1]+result_columns[2:3]+result_columns[4:5]+result_columns[3:4] \\\n",
    "    +result_columns[5:6]+result_columns[6:7]+result_columns[10:11]+result_columns[7:8]+result_columns[11:12]+result_columns[8:9] \\\n",
    "    +result_columns[12:13]+result_columns[9:10]+result_columns[13:]\n",
    "\n",
    "    em_name_match_df=em_name_match_df[columns_sort].sort_values(by='similarity', ascending=False)\n",
    "    \n",
    "    # saving results based on type of dataframe loaded into the function\n",
    "    \n",
    "    if employee_status == 'active':\n",
    "        em_name_match_df.to_csv('active_em_name_match.csv', index=False)\n",
    "        r=em_name_match_df.copy()\n",
    "    else:\n",
    "        em_name_match_df.to_csv('terminated_em_name_match.csv', index=False)\n",
    "        r=em_name_match_df.copy()\n",
    "        \n",
    "    return r\n",
    "\n",
    "def weekday_or_weekend(date_col) :\n",
    "    '''\n",
    "    converting timestamp column to day number\n",
    "    '''\n",
    "    return dt.datetime.weekday(date_col)\n",
    "\n",
    "# setting up weekend days based on day numbers\n",
    "# Week starts on Monday: Monday is o - Sunday is 6\n",
    "weekends = [4,5]\n",
    "\n",
    "# setting up abnormal working hours\n",
    "abnormal_working_hours = [20, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#  vendor records exact and fuzzy matches\n",
    "\n",
    "# finding exact and fuzzy matches between vendor names to identify possible duplicates\n",
    "# based on user specified n_gram and number of desired matches 'n_matches'.\n",
    "# n_gram and n_matches default values are 3 and 10 respectively which mostly yield best results\n",
    "# results are not filtered but are sorted in a descending order for convenience.\n",
    "\n",
    "# preparing list of names to match\n",
    "vendor_name = vmf_df.name.tolist()\n",
    "\n",
    "match_name = vmf_df.name.tolist()\n",
    "\n",
    "# applying match function, adjusting data view and calculating similarities of returned results\n",
    "vn_name_match_df=tm.matcher(vendor_name , match_name, ngram_length= n_gram, k_matches= n_matches)\n",
    "\n",
    "vn_name_match_df=pd.merge(vn_name_match_df,vmf_df[['id']], left_index=True,right_index=True,how='left')\n",
    "\n",
    "vn_name_match_df.drop_duplicates(subset='Original Name', inplace=True)\n",
    "\n",
    "vn_name_match_df.drop(columns=['Match Confidence','Lookup 1'], inplace=True)\n",
    "\n",
    "vn_name_match_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "vn_name_match_df=vn_name_match_df.melt(id_vars=[\"Original Name\", 'id'],ignore_index=False)\n",
    "\n",
    "vn_name_match_df.drop(columns=['variable'], inplace=True)\n",
    "\n",
    "vn_name_match_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "vn_name_match_df['similarity']=vn_name_match_df.apply(similar,args=('Original Name', 'value'),axis=1)\n",
    "\n",
    "vn_name_match_df.index=vn_name_match_df.value\n",
    "\n",
    "vmf_df.index=vmf_df.name\n",
    "\n",
    "# collecting vendor details for matched vendors\n",
    "vn_name_match_df=pd.merge(vn_name_match_df,vmf_df[['id','vendor_status']], left_index=True,right_index=True,how='left')\n",
    "\n",
    "vn_name_match_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "vmf_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# remove duplicate records\n",
    "temp_df=vn_name_match_df.groupby(['id_x','Original Name', 'id_y', 'value', 'similarity', 'vendor_status'])['value'].count().to_frame()\n",
    "\n",
    "vn_name_match_df= temp_df.index.to_frame()\n",
    "\n",
    "vn_name_match_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# remove exact id matches\n",
    "vn_name_match_df=vn_name_match_df[vn_name_match_df.id_x != vn_name_match_df.id_y]\n",
    "\n",
    "vn_name_match_df[['id_x','id_y']] = vn_name_match_df[['id_x','id_y']].astype(str)\n",
    "\n",
    "# removing reverse duplicate matches to eliminate data redundancy, \n",
    "# i.e: match result of x,y and y,x basically refer to same records\n",
    "# first we join and sort both vendor ids to create a unique value for each result\n",
    "# then drop duplicate joined ids\n",
    "vn_name_match_df['reverse_duplicates'] = vn_name_match_df.apply(lambda row: ''.join(sorted([row.id_x, row.id_y])), axis=1)\n",
    "\n",
    "vn_name_match_df.drop_duplicates(subset='reverse_duplicates', inplace=True)\n",
    "\n",
    "vn_name_match_df.drop(columns='reverse_duplicates', inplace=True)\n",
    "\n",
    "vn_name_match_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# collecting all match vendor details and sorting data for better view and comparison\n",
    "vn_name_match_df[['id_x','id_y']] = vn_name_match_df[['id_x','id_y']].astype(float)\n",
    "\n",
    "vn_name_match_df.rename(columns={'id_x':'id','Original Name':'name'}, inplace=True)\n",
    "\n",
    "vn_name_match_df=pd.merge(vn_name_match_df,vmf_df[['id', 'phone', 'postal_code', 'address', \n",
    "                                                   'taxpayer_identification_number_tin']], on='id', how='left')\n",
    "\n",
    "vn_name_match_df.rename(columns={'id':'vendor_id','name':'vendor_name','phone':'vendor_phone', \n",
    "                                 'postal_code':'vendor_postal_code', 'address':'vendor_address', \n",
    "                                 'taxpayer_identification_number_tin':'vendor_tin'}, inplace=True)\n",
    "\n",
    "vn_name_match_df.rename(columns={'id_y':'id','value':'name'}, inplace=True)\n",
    "\n",
    "vn_name_match_df=pd.merge(vn_name_match_df,vmf_df[['id', 'phone', 'postal_code', 'address', \n",
    "                                                   'taxpayer_identification_number_tin']], on='id', how='left')\n",
    "\n",
    "vn_name_match_df.rename(columns={'id':'match_vendor_id', 'name':'match_vendor_name', 'phone':'match_vendor_phone', \n",
    "                                 'postal_code':'match_vendor_postal_code', 'address':'match_vendor_address', \n",
    "                                 'taxpayer_identification_number_tin':'match_vendor_tin'}, inplace=True)\n",
    "\n",
    "result_columns=vn_name_match_df.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[:2]+result_columns[3:4]+result_columns[2:3]+result_columns[4:6]+result_columns[6:7] \\\n",
    "+result_columns[10:11]+result_columns[7:8]+result_columns[11:12]+result_columns[8:9]+result_columns[12:13] \\\n",
    "+result_columns[9:10]+result_columns[13:]\n",
    "\n",
    "vn_name_match_df=vn_name_match_df[columns_sort].sort_values(by='similarity', ascending=False)\n",
    "\n",
    "r1=vn_name_match_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Active employees records vs vendors records\n",
    "\n",
    "r2 = employee_vs_vendor_records(vmf_df, employees_df, n_gram, n_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Terminated employees records vs vendors records\n",
    "\n",
    "r3 = employee_vs_vendor_records(vmf_df, terminated_employees_df, n_gram, n_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# lang_filter\n",
    "\n",
    "r4 = vmf_df[~vmf_df['name'].apply(non_english_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# POs placed to employees\n",
    "# identify all POs issued in the name of employees either active or terminated,\n",
    "# using the results from previous functions and linking to po list\n",
    "# both exact and fuzzy name matches are considered\n",
    "\n",
    "active = pd.read_csv('active_em_name_match.csv')\n",
    "\n",
    "terminated = pd.read_csv('terminated_em_name_match.csv')\n",
    "\n",
    "# building the dataframe\n",
    "# filtering close matches, according to difflib official documentation\n",
    "# value over 0.6 means the sequences are close matches\n",
    "fltrd_active_temp_df=active[active.similarity >= .6].copy()\n",
    "\n",
    "fltrd_active_temp_df['employee_status']= 'Active'\n",
    "\n",
    "fltrd_term_temp_df= terminated[terminated.similarity >= .6].copy()\n",
    "\n",
    "fltrd_term_temp_df['employee_status']= 'Terminated'\n",
    "\n",
    "employee_vs_po_list= fltrd_active_temp_df.append(fltrd_term_temp_df, ignore_index=True)\n",
    "\n",
    "# mapping termination date to terminated employees\n",
    "for i in employee_vs_po_list.employee_name.unique():\n",
    "    \n",
    "    term_date_dict=terminated_employees_df.set_index('employee_name').to_dict()['termination_date']\n",
    "    \n",
    "    if i in terminated_employees_df.employee_name.values:\n",
    "        \n",
    "        employee_vs_po_list['termination_date']=employee_vs_po_list.employee_name.map(term_date_dict)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        employee_vs_po_list['termination_date']= 'N/A'\n",
    "            \n",
    "employee_vs_po_list=pd.merge(employee_vs_po_list,po_df[['vendor_name','po_number','po_date','po_status','po_total', 'currency']], \n",
    "                             on='vendor_name', how='left')\n",
    "\n",
    "# remove records of employees having no po issued in thier name\n",
    "temp_df=employee_vs_po_list.groupby(['employee_id', 'employee_name', 'vendor_name', 'vendor_id','similarity', \n",
    "                                     'vendor_status', 'employee_status', 'termination_date','po_date', 'po_number',\n",
    "                                     'po_status', 'po_total', 'currency'])['po_number'].count().to_frame()\n",
    "\n",
    "employee_vs_po_list= temp_df.index.to_frame()\n",
    "\n",
    "employee_vs_po_list.reset_index(drop=True, inplace=True)\n",
    "\n",
    "r5=employee_vs_po_list.sort_values(by='similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# access rights review\n",
    "# identify any unauthorized record manipulation\n",
    "# by comparing edit history to approved access rights and employee records\n",
    "\n",
    "# building the dataframe\n",
    "creation_temp_df = vmf_df[~vmf_df.creation_user_id.isin(access_rights_df.creation_user_id)]\n",
    "\n",
    "modification_temp_df = vmf_df[~vmf_df.modification_user_id.isin(access_rights_df.modification_user_id)]\n",
    "\n",
    "access_rights_review_df = creation_temp_df.append(modification_temp_df)\n",
    "\n",
    "access_rights_review_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "employees_df['employee_status'] = 'Active'\n",
    "\n",
    "terminated_employees_df['employee_status'] = 'Terminated'\n",
    "\n",
    "# adjusting column length to exact match in both dataframes\n",
    "fltrt_term_df=terminated_employees_df[employees_df.columns]\n",
    "\n",
    "temp_employee_df=employees_df.append(fltrt_term_df)\n",
    "\n",
    "# collecting all active/terminated employee details and sorting data for better view and comparison\n",
    "access_rights_review_df.index=access_rights_review_df.creation_user_id\n",
    "\n",
    "temp_employee_df.index=temp_employee_df.employee_id\n",
    "\n",
    "access_rights_review_df=pd.merge(access_rights_review_df,temp_employee_df[['departement', 'employee_status', 'employee_name']],\n",
    "                                 left_index=True, right_index=True, how='left')\n",
    "\n",
    "access_rights_review_df.rename(columns={'departement':'creation_user_departement', 'employee_status':'creation_user_status', \n",
    "                                        'employee_name':'creation_user_name'}, inplace=True)\n",
    "\n",
    "access_rights_review_df.index=access_rights_review_df.modification_user_id\n",
    "\n",
    "access_rights_review_df=pd.merge(access_rights_review_df,temp_employee_df[['departement', 'employee_status', 'employee_name']],\n",
    "                                 left_index=True, right_index=True, how='left')\n",
    "\n",
    "access_rights_review_df.rename(columns={'departement':'modification_user_departement', \n",
    "                                        'employee_status':'modification_user_status', 'employee_name':'modification_user_name',\n",
    "                                        'id':'vendor_id', 'name':'vendor_name'}, inplace=True)\n",
    "\n",
    "# mapping termination date to terminated employees\n",
    "for i in access_rights_review_df.creation_user_id.unique():\n",
    "    term_date_dict=terminated_employees_df.set_index('employee_id').to_dict()['termination_date']\n",
    "    \n",
    "    if i in terminated_employees_df.employee_id.values:\n",
    "        access_rights_review_df['termination_date']=access_rights_review_df.creation_user_id.map(term_date_dict)\n",
    "    else:\n",
    "        access_rights_review_df['termination_date']=access_rights_review_df.modification_user_id.map(term_date_dict)\n",
    "\n",
    "access_rights_review_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "temp_employee_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "result_columns=access_rights_review_df.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[0:3]+result_columns[9:10]+result_columns[11:12]+result_columns[14:15]+result_columns[17:18] \\\n",
    "+result_columns[19:]+result_columns[10:11]+result_columns[13:14]+result_columns[15:16]+result_columns[12:13] \\\n",
    "+result_columns[16:17]+result_columns[18:19]\n",
    "\n",
    "access_rights_review_df=access_rights_review_df[columns_sort]\n",
    "\n",
    "r6=access_rights_review_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Employees editing thier own vendor records\n",
    "# both exact and fuzzy name matches are considered\n",
    "# results are filtered to the nearest match\n",
    "\n",
    "access_rights_review_df['similarity_creation'] = access_rights_review_df.apply(similar,args=('vendor_name', 'creation_user_name'),axis=1)\n",
    "\n",
    "access_rights_review_df['similarity_modification'] = access_rights_review_df.apply(similar,args=('vendor_name', 'modification_user_name'),axis=1)\n",
    "\n",
    "r7=access_rights_review_df[(access_rights_review_df.similarity_creation >= .6) | \n",
    "                           (access_rights_review_df.similarity_modification >= .6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Weekends modifications and at abnormal working hours\n",
    "\n",
    "temp_vmf_df=vmf_df.copy()\n",
    "\n",
    "temp_vmf_df['creation_hour'] = temp_vmf_df['creation_date'].dt.hour\n",
    "\n",
    "temp_vmf_df['modification_hour'] = temp_vmf_df['modification_date'].dt.hour\n",
    "    \n",
    "temp_vmf_df['creation_day']=temp_vmf_df.creation_date.apply(weekday_or_weekend)\n",
    "\n",
    "temp_vmf_df['modification_day']=temp_vmf_df.modification_date.apply(weekday_or_weekend)\n",
    "\n",
    "# modifications on weekends\n",
    "r8 = temp_vmf_df[(temp_vmf_df.creation_day.isin(weekends)) | (temp_vmf_df.modification_day.isin(weekends))]\n",
    "\n",
    "result_columns=r8.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[0:3]+result_columns[9:]\n",
    "\n",
    "r8 = r8[columns_sort]\n",
    "\n",
    "# late modifications after normal working hours,\n",
    "# excluding records already identified in weekend modification to avoid duplication\n",
    "if len(abnormal_working_hours) > 1 :\n",
    "    if abnormal_working_hours[0] in np.arange(12,24):\n",
    "    \n",
    "        temp_creation_df = temp_vmf_df[(temp_vmf_df.creation_hour >= abnormal_working_hours[0]) | \n",
    "                                       (temp_vmf_df.creation_hour <= abnormal_working_hours[1])]\n",
    "\n",
    "        temp_modification_df = temp_vmf_df[(temp_vmf_df.modification_hour >= abnormal_working_hours[0]) | \n",
    "                                           (temp_vmf_df.modification_hour <= abnormal_working_hours[1])]\n",
    "    else:\n",
    "    \n",
    "        temp_creation_df = temp_vmf_df[(temp_vmf_df.creation_hour >= abnormal_working_hours[0]) & \n",
    "                                       (temp_vmf_df.creation_hour <= abnormal_working_hours[1])]\n",
    "\n",
    "        temp_modification_df = temp_vmf_df[(temp_vmf_df.modification_hour >= abnormal_working_hours[0]) & \n",
    "                                           (temp_vmf_df.modification_hour <= abnormal_working_hours[1])]\n",
    "else:\n",
    "    temp_creation_df = temp_vmf_df[(temp_vmf_df.creation_hour == abnormal_working_hours[0])]\n",
    "\n",
    "    temp_modification_df = temp_vmf_df[(temp_vmf_df.modification_hour == abnormal_working_hours[0])]\n",
    "    \n",
    "# filter out records already identified as abnormal creation hour\n",
    "temp_modification_df = temp_modification_df[~temp_modification_df.id.isin(temp_creation_df.id)]\n",
    "\n",
    "r9 = temp_creation_df.append(temp_modification_df)\n",
    "\n",
    "r9.reset_index(drop=True, inplace=True)\n",
    "\n",
    "result_columns=r9.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[0:3]+result_columns[9:]\n",
    "\n",
    "r9 = r9[columns_sort]\n",
    "\n",
    "# filter out records already identified as weekend modification\n",
    "r9 = r9[~r9.id.isin(r8.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 188 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# PO issued to inactive vendors\n",
    "\n",
    "temp_po_df=po_df.copy()\n",
    "\n",
    "# mapping vendor status to po list\n",
    "vendor_status_dict=vmf_df.set_index('name').to_dict()['vendor_status']\n",
    "\n",
    "temp_po_df['vendor_status']=temp_po_df.vendor_name.map(vendor_status_dict)\n",
    "\n",
    "vmf_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "r10 = temp_po_df[temp_po_df.vendor_status == 'In-Active'].sort_values(by='po_total', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gaps in vendor id/po number sequential order and duplicate records\n",
    "\n",
    "# building dataframe: vendor records\n",
    "\n",
    "vendor_gaps_df=vmf_df.copy().sort_values(by='id')\n",
    "\n",
    "vendor_gaps_df['start'] = vendor_gaps_df['id']\n",
    "\n",
    "vendor_gaps_df['end'] = vendor_gaps_df['id'].shift(-1)\n",
    "\n",
    "vendor_gaps_df['gap'] = vendor_gaps_df['id'].shift(-1) - vendor_gaps_df['id']\n",
    "\n",
    "\n",
    "# building dataframe: po records\n",
    "\n",
    "po_gaps_df=po_df.copy().sort_values(by='po_number')\n",
    "\n",
    "po_gaps_df['start'] = po_gaps_df['po_number']\n",
    "\n",
    "po_gaps_df['end'] = po_gaps_df['po_number'].shift(-1)\n",
    "\n",
    "po_gaps_df['gap'] = po_gaps_df['po_number'].shift(-1) - po_gaps_df['po_number']\n",
    "\n",
    "\n",
    "# vendor id gaps\n",
    "r11 = vendor_gaps_df[(vendor_gaps_df['gap'] >0) & (vendor_gaps_df['gap'] != 1)]\n",
    "\n",
    "# vendor id duplicates\n",
    "r12 = vendor_gaps_df[(vendor_gaps_df['gap'] == 0)].dropna()\n",
    "\n",
    "# po number gaps\n",
    "r13 = po_gaps_df[(po_gaps_df['gap'] >0) & (po_gaps_df['gap'] != 1)]\n",
    "\n",
    "# po number duplicates\n",
    "r14 = po_gaps_df[(po_gaps_df['gap'] == 0)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching vendor_phone with match_vendor_phone\n",
      "matching vendor_postal_code with match_vendor_postal_code\n",
      "matching vendor_address with match_vendor_address\n",
      "matching vendor_tin with match_vendor_tin\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# similarity across all vendor data\n",
    "# considering only highest possible match\n",
    "\n",
    "temp_vn_df = r1.copy()\n",
    "\n",
    "temp_vn_df = temp_vn_df[temp_vn_df.similarity >= .6]\n",
    "\n",
    "for i, e in zip(*[iter(temp_vn_df.columns[6:])] * 2):\n",
    "    print('matching' + ' ' + i + ' ' + 'with' + ' ' + e)\n",
    "    result = temp_vn_df.apply(similar_with_nan,args=(i,e),axis=1)\n",
    "    temp_vn_df[i] = result\n",
    "    temp_vn_df.drop(columns=e,inplace=True)\n",
    "\n",
    "temp_vn_df['total_similarity_score'] = temp_vn_df['similarity'] + temp_vn_df.iloc[:,-5:-1].sum(axis=1)\n",
    "\n",
    "r15 = temp_vn_df.copy().sort_values(by='total_similarity_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching employee_phone with vendor_phone\n",
      "matching employee_postal_code with vendor_postal_code\n",
      "matching employee_address with vendor_address\n",
      "matching employee_ssn with vendor_tin\n",
      "Wall time: 808 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# similarity across all active employee vs vendor data\n",
    "# considering only highest possible match\n",
    "\n",
    "temp_emp_df=r2.copy()\n",
    "\n",
    "temp_emp_df=temp_emp_df[temp_emp_df.similarity >= .6]\n",
    "\n",
    "for i, e in zip(*[iter(temp_emp_df.columns[6:])] * 2):\n",
    "    print('matching' + ' ' + i + ' ' + 'with' + ' ' + e)\n",
    "    result=temp_emp_df.apply(similar_with_nan,args=(i,e),axis=1)\n",
    "    temp_emp_df[i]= result\n",
    "    temp_emp_df.drop(columns=e,inplace=True)\n",
    "\n",
    "temp_emp_df['total_similarity_score'] = temp_emp_df['similarity'] + temp_emp_df.iloc[:,-5:-1].sum(axis=1)\n",
    "\n",
    "r16 = temp_emp_df.copy().sort_values(by='total_similarity_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching employee_phone with vendor_phone\n",
      "matching employee_postal_code with vendor_postal_code\n",
      "matching employee_address with vendor_address\n",
      "matching employee_ssn with vendor_tin\n",
      "Wall time: 742 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# similarity across all terminated employee vs vendor data\n",
    "# considering only highest possible match\n",
    "\n",
    "temp_termin_df=r3.copy()\n",
    "\n",
    "temp_termin_df=temp_termin_df[temp_termin_df.similarity >= .6]\n",
    "\n",
    "for i, e in zip(*[iter(temp_termin_df.columns[6:])] * 2):\n",
    "    print('matching' + ' ' + i + ' ' + 'with' + ' ' + e)\n",
    "    result=temp_termin_df.apply(similar_with_nan,args=(i,e),axis=1)\n",
    "    temp_termin_df[i]= result\n",
    "    temp_termin_df.drop(columns=e,inplace=True)\n",
    "\n",
    "temp_termin_df['total_similarity_score'] = temp_termin_df['similarity'] + temp_termin_df.iloc[:,-5:-1].sum(axis=1)\n",
    "\n",
    "r17 = temp_termin_df.copy().sort_values(by='total_similarity_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 451 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# terminated employees with highest number of POs placed after termination date\n",
    "# exact matches are only considered\n",
    "\n",
    "temp_term_df=r5.copy()\n",
    "\n",
    "r18=temp_term_df[(temp_term_df.termination_date) < (temp_term_df.po_date)]\n",
    "\n",
    "# filter for exact matches only\n",
    "\n",
    "fltrd_r18=r18[r18.similarity == 1].copy()\n",
    "\n",
    "earliest_po_date = fltrd_r18.groupby(['employee_id', 'employee_name', 'termination_date'])['po_date'].min().to_frame()\n",
    "\n",
    "po_count_df = fltrd_r18.groupby(['employee_id', 'employee_name', 'termination_date'])['po_number'].count().to_frame()\n",
    "\n",
    "po_value_df = fltrd_r18.groupby(['employee_id', 'employee_name', 'termination_date'])['po_total'].sum().to_frame()\n",
    "\n",
    "df_list = [earliest_po_date, po_count_df, po_value_df, earliest_po_date.index.to_frame()]\n",
    "\n",
    "r18_summary = reduce(lambda  left,right: pd.merge(left,right, left_index=True, right_index=True,\n",
    "                                                 how='outer'), df_list).rename(columns={'po_date' : 'earliest_po_date', \n",
    "                                                                                        'po_number' : 'po_count', \n",
    "                                                                                        'po_total' : 'sum_po_values'})\n",
    "\n",
    "result_columns=r18_summary.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[3:]+result_columns[0:1]+result_columns[1:3]\n",
    "\n",
    "r18_summary = r18_summary[columns_sort].sort_values(by='sum_po_values', ascending=False)\n",
    "\n",
    "r18_summary.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='missing_records'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# missing vendor details\n",
    "\n",
    "missing_details_count = vmf_df.isna().sum()\n",
    "\n",
    "r19 = missing_details_count[missing_details_count > 0].to_frame()\n",
    "\n",
    "r19 = pd.merge(r19.index.to_frame(), r19, left_index=True, right_index=True, \n",
    "               how='left').rename(columns={'0_x':'missing_records', '0_y':'missing_records_count'})\n",
    "\n",
    "r19.reset_index(drop=True, inplace=True)\n",
    "\n",
    "r19.plot(kind='barh', x= 'missing_records', y='missing_records_count', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# summary of po details for inactive vendors \n",
    "\n",
    "temp_po_df= r10.copy()\n",
    "\n",
    "po_count = temp_po_df.groupby('vendor_name')['po_number'].count().to_frame()\n",
    "\n",
    "po_value = temp_po_df.groupby(['vendor_name', 'currency'])['po_total'].sum().to_frame()\n",
    "\n",
    "df_list = [po_value, po_count, po_count.index.to_frame()]\n",
    "\n",
    "r20 = reduce(lambda  left,right: pd.merge(left,right, left_index=True, right_index=True,\n",
    "                                          how='outer'), df_list).rename(columns={'po_number': 'po_count',\n",
    "                                                                                 'po_total': 'sum_po_values'})\n",
    "\n",
    "r20.reset_index(drop=True, inplace=True)\n",
    "\n",
    "result_columns=r20.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[2:]+result_columns[1:2]+result_columns[0:1]\n",
    "\n",
    "r20 = r20[columns_sort].sort_values(by='sum_po_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# summary of weekend and abnormal working hours modifications \n",
    "\n",
    "# weekends modification\n",
    "\n",
    "if len(weekends) > 1 :\n",
    "    temp_days_creation = r8[(r8.creation_day == weekends[0]) | (r8.creation_day == weekends[1])]\n",
    "\n",
    "    temp_days_modification = r8[(r8.modification_day == weekends[0]) | (r8.modification_day == weekends[1])]\n",
    "else:\n",
    "    temp_days_creation = r8[(r8.creation_day == weekends[0])]\n",
    "\n",
    "    temp_days_modification = r8[(r8.modification_day == weekends[0])]\n",
    "\n",
    "weekends_creation = temp_days_creation.groupby(['creation_user_id'])['creation_day'].count().to_frame()\n",
    "\n",
    "weekends_modification = temp_days_modification.groupby(['modification_user_id'])['modification_day'].count().to_frame()\n",
    "\n",
    "r21 = pd.merge(weekends_creation, weekends_creation.index.to_frame(), left_index=True, \n",
    "               right_index=True, how='left').rename(columns={'creation_day': 'created_records_count'})\n",
    "\n",
    "r22 = pd.merge(weekends_modification, weekends_modification.index.to_frame(), left_index=True, \n",
    "               right_index=True, how='left').rename(columns={'modification_day': 'modified_records_count'})\n",
    "\n",
    "r21['user_authorized'] = r21.creation_user_id.isin(access_rights_df.creation_user_id)\n",
    "\n",
    "r22['user_authorized'] = r22.modification_user_id.isin(access_rights_df.modification_user_id)\n",
    "\n",
    "result_columns=r21.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[1:2]+result_columns[0:1]+result_columns[2:]\n",
    "\n",
    "r21 = r21[columns_sort].sort_values(by='created_records_count', ascending = False)\n",
    "\n",
    "result_columns=r22.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[1:2]+result_columns[0:1]+result_columns[2:]\n",
    "\n",
    "r22 = r22[columns_sort].sort_values(by='modified_records_count', ascending = False)\n",
    "\n",
    "r21.reset_index(drop=True, inplace=True)\n",
    "r22.reset_index(drop=True, inplace=True)\n",
    "\n",
    "r23 = pd.merge(r21,r22, left_index=True, right_index=True, \n",
    "            how='outer').rename(columns={'user_authorized_x': 'creation_user_authorized',\n",
    "                                         'user_authorized_y' : 'modification_user_authorized'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 84 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# summary abnormal working hours modification\n",
    "\n",
    "abnormal_creation = temp_creation_df.groupby(['creation_user_id'])['creation_hour'].count().to_frame()\n",
    "abnormal_modification = temp_modification_df.groupby(['modification_user_id'])['modification_hour'].count().to_frame()\n",
    "\n",
    "r24 = pd.merge(abnormal_creation, abnormal_creation.index.to_frame(), left_index=True, \n",
    "               right_index=True, how='left').rename(columns={'creation_hour': 'created_records_count'})\n",
    "\n",
    "r25 = pd.merge(abnormal_modification, abnormal_modification.index.to_frame(), left_index=True, \n",
    "               right_index=True, how='left').rename(columns={'modification_hour': 'modified_records_count'})\n",
    "\n",
    "r24['user_authorized'] = r24.creation_user_id.isin(access_rights_df.creation_user_id)\n",
    "\n",
    "r25['user_authorized'] = r25.modification_user_id.isin(access_rights_df.modification_user_id)\n",
    "\n",
    "result_columns=r24.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[1:2]+result_columns[0:1]+result_columns[2:]\n",
    "\n",
    "r24 = r24[columns_sort].sort_values(by='created_records_count', ascending = False)\n",
    "\n",
    "result_columns=r25.columns.tolist()\n",
    "\n",
    "columns_sort=result_columns[1:2]+result_columns[0:1]+result_columns[2:]\n",
    "\n",
    "r25 = r25[columns_sort].sort_values(by='modified_records_count', ascending = False)\n",
    "\n",
    "r24.reset_index(drop=True, inplace=True)\n",
    "r25.reset_index(drop=True, inplace=True)\n",
    "\n",
    "r26 = pd.merge(r24,r25, left_index=True, right_index=True, \n",
    "            how='outer').rename(columns={'user_authorized_x': 'creation_user_authorized',\n",
    "                                         'user_authorized_y' : 'modification_user_authorized'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 456 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# summary of modification by period\n",
    "\n",
    "temp_period_df = vmf_df.copy()\n",
    "\n",
    "creation = temp_period_df.groupby(['creation_date'])['creation_date'].count().resample('1y').sum().to_frame()\n",
    "\n",
    "modification = temp_period_df.groupby(['modification_date'])['modification_date'].count().resample('1y').sum().to_frame()\n",
    "\n",
    "creation = creation[creation.creation_date > 0]\n",
    "\n",
    "modification = modification[modification.modification_date > 0]\n",
    "\n",
    "r27 = pd.merge(creation, modification, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "r27 = pd.merge(r27.index.to_frame(), r27, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "r27.rename(columns={0: 'period', 'creation_date' : 'created_records_count', \n",
    "                    'modification_date' : 'modified_records_count'}, inplace=True)\n",
    "\n",
    "r27.reset_index(drop=True, inplace=True)\n",
    "\n",
    "r27= r27[(r27.created_records_count >= 0) | (r27.modified_records_count >= 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# summary of similarity across all vendor data,\n",
    "# only exact and highest total similarities are considered,\n",
    "# score of 2 and above provided best result filter.\n",
    "\n",
    "r28 = r15[r15.total_similarity_score >= 2]\n",
    "\n",
    "r28 = r28[r28.columns[0:4]]\n",
    "\n",
    "r28.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# summary of similarity across all active employee vs vendor data,\n",
    "# only exact and highest total similarities are considered,\n",
    "# score of 1.5 and above provided best result filter.\n",
    "\n",
    "r29 = r16[(r16.similarity == 1) | (r16.total_similarity_score >= 1.5)]\n",
    "\n",
    "r29 = r29[r29.columns[0:4]]\n",
    "\n",
    "r29.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# summary of similarity across all terminated employee vs vendor data, \n",
    "# only exact and highest total similarities are considered,\n",
    "# score of 1.5 and above provided best result filter.\n",
    "\n",
    "r30 = r17[(r17.similarity == 1) | (r17.total_similarity_score >= 1.5)]\n",
    "\n",
    "r30 = r30[r30.columns[0:4]]\n",
    "\n",
    "r30.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Saving Results\n",
    "\n",
    "writer = pd.ExcelWriter('VMF_Analysed.xlsx', engine='xlsxwriter')\n",
    "\n",
    "workbook = writer.book\n",
    "\n",
    "worksheet = workbook.add_worksheet('summary_tables')\n",
    "\n",
    "writer.sheets['summary_tables'] = worksheet\n",
    "\n",
    "worksheet.hide_gridlines(2)\n",
    "\n",
    "form = workbook.add_format()\n",
    "\n",
    "form.set_align('center')\n",
    "\n",
    "form.set_align('vcenter')\n",
    "\n",
    "worksheet.set_column('A:G', 30, form)\n",
    "\n",
    "chart = workbook.add_chart({'type': 'column'})\n",
    "\n",
    "# Configure the series of the chart from the dataframe data.\n",
    "chart.add_series({'categories': '=summary_tables!$A$3:$A$7',\n",
    "                  'values': '=summary_tables!$B$3:$B$7',\n",
    "                  'fill':   {'color': 'brown'},\n",
    "                  'line': {'color': 'black'}})\n",
    "\n",
    "# Insert the chart into the worksheet.\n",
    "worksheet.insert_chart('C1', chart, {'x_scale': 1, 'y_scale': .5, 'x_offset': 25, 'y_offset': 10})\n",
    "\n",
    "chart.set_legend({'position': 'none'})\n",
    "\n",
    "# Configure the chart axes.\n",
    "chart.set_x_axis({'name': 'Missing Data'})\n",
    "chart.set_y_axis({'name': 'Count', 'major_gridlines': {'visible': False}})\n",
    "\n",
    "worksheet.write(0, 0, 'Missing_vendor details')\n",
    "\n",
    "r19.to_excel(writer, sheet_name = 'summary_tables', startrow=1 , startcol=0, index = False)\n",
    "\n",
    "worksheet.write(r19.shape[0] + 3, 0, 'summary of vendors having highest similarities across all records')\n",
    "\n",
    "r28.to_excel(writer, sheet_name = 'summary_tables', startrow= r19.shape[0] + 4 , startcol=0, index = False)\n",
    "\n",
    "worksheet.write(r19.shape[0] + 3 + r28.shape[0] + 3, 0, \n",
    "                'summary of active employees and vendors having highest similarities across all records')\n",
    "\n",
    "r29.to_excel(writer, sheet_name = 'summary_tables', startrow= r19.shape[0] + 3 + r28.shape[0] + 4 , startcol=0, \n",
    "             index = False)\n",
    "\n",
    "worksheet.write(r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3, 0, \n",
    "                'summary of terminated employees and vendors having highest similarities across all records')\n",
    "\n",
    "r30.to_excel(writer, sheet_name = 'summary_tables', startrow= r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 4 , \n",
    "             startcol=0, index = False)\n",
    "\n",
    "worksheet.write(r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 + r30.shape[0] + 3, 0, \n",
    "                'Issued POs after employee termination date, only exact name matches are considered')\n",
    "\n",
    "r18_summary.to_excel(writer, sheet_name = 'summary_tables', startrow= r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] \n",
    "                     + 3 + r30.shape[0] + 4, startcol=0, index = False)\n",
    "\n",
    "worksheet.write(r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 + r30.shape[0] + 3 + r18_summary.shape[0] + 3, 0, \n",
    "                'Issued POs for inactive vendor')\n",
    "\n",
    "r20.to_excel(writer, sheet_name = 'summary_tables', startrow= r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 \n",
    "             + r30.shape[0] + 3 + r18_summary.shape[0] + 4 , startcol=0, index = False)\n",
    "\n",
    "worksheet.write(r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 + r30.shape[0] + 3 + r18_summary.shape[0] + 3 \n",
    "                + r20.shape[0] + 3, 0, 'Vendor records creation/modification on weekends')\n",
    "\n",
    "r23.to_excel(writer, sheet_name = 'summary_tables', startrow= r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 \n",
    "             + r30.shape[0] + 3 + r18_summary.shape[0] + 3 + r20.shape[0] + 4 , startcol=0, index = False)\n",
    "\n",
    "worksheet.write(r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 + r30.shape[0] + 3 + r18_summary.shape[0] + 3 \n",
    "                + r20.shape[0] + 3 + r23.shape[0] + 3, 0, 'Vendor records creation/modification at abnormal working hours')\n",
    "\n",
    "r26.to_excel(writer, sheet_name = 'summary_tables', startrow= r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 \n",
    "             + r30.shape[0] + 3 + r18_summary.shape[0] + 3 + r20.shape[0] + 3 + r23.shape[0] + 4 , startcol=0, \n",
    "             index = False)\n",
    "\n",
    "worksheet.write(r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 + r30.shape[0] + 3 + r18_summary.shape[0] + 3 \n",
    "                + r20.shape[0] + 3 + r23.shape[0] + 3 + r26.shape[0] + 3, 0, \n",
    "                ' Summary of vendor records creation/modification by period')\n",
    "\n",
    "r27.to_excel(writer, sheet_name = 'summary_tables', startrow= r19.shape[0] + 3 + r28.shape[0] + 3 + r29.shape[0] + 3 \n",
    "             + r30.shape[0] + 3 + r18_summary.shape[0] + 3 + r20.shape[0] + 3 + r23.shape[0] + 3 + r26.shape[0] + 4 , \n",
    "             startcol=0, index = False)\n",
    "\n",
    "r1.to_excel(writer, sheet_name = 'vendor_name_match', index = False)\n",
    "\n",
    "r2.to_excel(writer, sheet_name = 'active_emp_vs_ven_name_match', index = False)\n",
    "\n",
    "r3.to_excel(writer, sheet_name = 'term_emp_vs_ven_name_match', index = False)\n",
    "\n",
    "r4.to_excel(writer, sheet_name = 'non_english_ven_names', index = False)\n",
    "\n",
    "r5.to_excel(writer, sheet_name = 'po_to_employees', index = False)\n",
    "\n",
    "r6.to_excel(writer, sheet_name = 'unauthorized_access', index = False)\n",
    "\n",
    "r7.to_excel(writer, sheet_name = 'employees_editing_own_records', index = False)\n",
    "\n",
    "r8.to_excel(writer, sheet_name = 'weekend_modifications', index = False)\n",
    "\n",
    "r9.to_excel(writer, sheet_name = 'abnormal_hours_modifications', index = False)\n",
    "\n",
    "r10.to_excel(writer, sheet_name = 'po_for_inactive_vendors', index = False)\n",
    "\n",
    "r11.to_excel(writer, sheet_name = 'gabs_vendor_id', index = False)\n",
    "\n",
    "r12.to_excel(writer, sheet_name = 'duplicate_vendor_id', index = False)\n",
    "\n",
    "r13.to_excel(writer, sheet_name = 'gabs_po_number', index = False)\n",
    "\n",
    "r14.to_excel(writer, sheet_name = 'duplicate_po_number', index = False)\n",
    "\n",
    "r15.to_excel(writer, sheet_name = 'similarity_all_vendor_details', index = False)\n",
    "\n",
    "r16.to_excel(writer, sheet_name = 'similarity_all_emp_ven_details', index = False)\n",
    "\n",
    "r17.to_excel(writer, sheet_name = 'similarity_all_term_ven_details', index = False)\n",
    "\n",
    "r18.to_excel(writer, sheet_name = 'po_date_after_emp_term_date', index = False)\n",
    "\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
